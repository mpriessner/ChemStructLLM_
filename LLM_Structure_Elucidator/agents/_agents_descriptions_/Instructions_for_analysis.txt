Below is an outline (in the same high-level style as your existing pipeline) of how you can add a new Visual Analysis / LLM Agent to your TARGET_ONLY workflow. This agent will do all the “post-processing” steps we’ve been discussing: combine the final set of candidate structures with their NMR data, produce or reference images, gather the predicted vs. experimental shifts, and call an LLM to generate explanatory commentary. I’ll also suggest how to structure the output files, and give an example of the prompts you might need for each sub-step.

1. Overview of the New Agent
Agent Name:

visual_analysis_agent.py (or something similar).
Primary Purpose:

Aggregate: Gather the final candidate structures, predicted and experimental NMR data, images, matching scores, etc.
Prepare: Format them into a coherent data package.
Explain: Call LLM(s) to analyze mismatches and produce human-readable commentary or “mini-reports.”
Output: Provide both a textual summary and any additional visuals or JSON references for further automation.
2. Where the New Agent Fits in the Workflow
Currently, your TARGET_ONLY workflow ends with step 9: “Visual Comparison Tool.” Instead:

(Optional) You can REPLACE the existing step #9 with the new visual_analysis_agent.

(OR) If you still want a purely graphical step, you can keep the existing “Visual Comparison Tool” for generating images, then add:

python
Copy code
WORKFLOW_STEPS['visual_analysis'] = VisualAnalysisAgent()
at step #10. The difference is:

The original tool might just be about plotting or generating images.
The new LLM-based agent is about deeper analysis, “explainability,” and final verdicts.
Example (adding it as step #10):

python
Copy code
WorkflowType.TARGET_ONLY: [
    # ... steps 1-9 ...
    WORKFLOW_STEPS['visual_comparison'],  # The old tool that just creates images
    WORKFLOW_STEPS['visual_analysis'],    # The new LLM-based agent
]
3. Implementation Outline
Below is a more detailed breakdown of tasks within this new agent and how you might structure them in code:

3.1. Gather All Final Data
Input:

A JSON (or dictionary) containing:
Candidate Structures (SMILES, InChI, etc.).
Predicted NMR (peak positions, intensities, coupling if available).
Experimental NMR (the same type of data).
Scoring Info from your existing pipeline (overall score, by-spectrum scores).
Molecule Images: either generated in a prior step or created on-demand here (with RDKit or other).
Implementation:

Typically, you’d load these from CandidateAnalysisTool output plus the final results from “Mol2Mol” or “MMST” steps.
Then store them in an internal data structure, e.g.:
python
Copy code
final_data = {
  "candidates": [
    {
      "smiles": "...",
      "predicted_nmr": {...},
      "matching_score": {...},
      "image_path": "path/to/png",
    },
    ...
  ],
  "experimental_nmr": {...}
}
3.2. Data Aggregation & Transformation
Goal: Provide each candidate’s predicted vs. experimental data in a convenient format:
Possibly produce a side-by-side table or JSON chunk:
python
Copy code
[
  {
    "spectrum_type": "1H",
    "predicted_peaks": [...],
    "experimental_peaks": [...],
    "errors": [...],
  },
  ...
]
Implementation:
You might create a helper function:
python
Copy code
def build_comparison_table(candidate, experimental_nmr):
    # returns a list of {spectrum_type, predicted_peaks, experimental_peaks, mismatch_info}
    ...
Attach these “tables” to each candidate’s entry.
3.3. LLM Prompting & Analysis
For Each Candidate:

Assemble a prompt that includes:

A short textual description of the molecule (SMILES or formula).
(Optional) A link or encoded version of the molecule image if your LLM can handle images (or a textual placeholder).
The predicted vs. experimental data in a structured format or short table.
Instructions: “Identify major mismatches,” “Explain if these differences are small enough to be normal variation or a sign the structure is incorrect,” etc.
Call the LLM using your chosen method (OpenAI API, local model, etc.). Something like:

python
Copy code
def analyze_candidate_with_llm(candidate_data):
    prompt = build_visual_analysis_prompt(candidate_data)
    response = llm_api_call(prompt)
    return response
Parse the LLM’s response or store it in raw text. If you want the LLM to produce structured JSON, you can ask it explicitly:

“Please respond in valid JSON with fields ‘major_mismatches’, ‘recommendation’, etc.”

Workflow:

You might do this in a loop:
python
Copy code
for candidate in final_data["candidates"]:
    candidate["llm_analysis"] = analyze_candidate_with_llm(candidate)
3.4. Summarize & Output
After analyzing all candidates with the LLM, generate a final report or JSON.
Example:
python
Copy code
def generate_final_summary(final_data):
    summary = "LLM Analysis Summary:\n"
    for candidate in final_data["candidates"]:
        summary += f"Candidate: {candidate['smiles']}\n"
        summary += f"LLM Comments: {candidate['llm_analysis']['major_comments']}\n"
        summary += "...\n\n"
    return summary
Write to disk or to your system’s knowledge base:
Possibly store a “human-readable” .txt or .md report + a structured .json with all the details.
4. Example Prompts for Each Sub-Step
Let’s assume for each candidate you want the LLM to analyze the data spectrum-by-spectrum. You could:

4.1. One Prompt per Spectrum
Prompt Template (example):

vbnet
Copy code
You are an NMR analysis assistant. We have a candidate molecule with SMILES: {smiles}.
Here is the predicted {spectrum_type} NMR data vs. experimental data:

Predicted:
{table_of_predicted_peaks}

Experimental:
{table_of_experimental_peaks}

Please:
1. Identify any major differences in chemical shift (e.g., >0.2 ppm for 1H).
2. Note if the number of peaks or intensity pattern is inconsistent.
3. Conclude whether the {spectrum_type} data suggests this structure is correct or questionable.
Respond in JSON with fields: "analysis", "suspicious_peaks", "verdict".
Then you do similarly for ^13C, HSQC, COSY, etc.

4.2. One Consolidated Prompt per Candidate
Alternatively, you can give all spectra at once:

sql
Copy code
You are an NMR analysis assistant. We have a candidate molecule with SMILES: {smiles}.
Below are tables of predicted vs. experimental data for 1H, 13C, HSQC, and COSY. 
Please produce an overall commentary on the structure’s consistency with the data, highlighting the largest mismatches.

1H NMR differences:
...

13C NMR differences:
...

HSQC correlations:
...

COSY correlations:
...

Reply in JSON:
{
  "overall_assessment": "Text summary",
  "flagged_discrepancies": ["..."],
  "recommendation": "Likely correct / Possibly incorrect"
}
Either method works—per-spectrum prompts can yield more granular detail, while a single combined prompt might be simpler to manage.

5. Files & Data Storage
Given your code structure, you might do:

visual_analysis_agent.py

Main agent class. Implements:
prepare_data() – collects final outputs from candidate_analysis, mol2mol, mmst.
run_analysis() – calls the LLM, processes responses.
generate_report() – merges all results into a final artifact (like a .json or .md).
config/prompts/visual_analysis_prompts.py (if you like organizing your prompts in a separate file)

A dictionary or string templates for each sub-step.
Output

visual_analysis_results.json in your results/ or _temp_folder/ directory, storing the LLM outputs.
Possibly a .md or .html final report with textual commentary and embedded images.
6. Example Revised Workflow
Below is a revised TARGET_ONLY workflow, showing how you might add the new step:

python
Copy code
WorkflowType.TARGET_ONLY: [
    WORKFLOW_STEPS['error_thresholds'],
    WORKFLOW_STEPS['retrosynthesis'],
    WORKFLOW_STEPS['nmr_simulation'],
    WORKFLOW_STEPS['peak_matching'],
    WORKFLOW_STEPS['forward_prediction'],
    WORKFLOW_STEPS['candidate_analysis'],
    WORKFLOW_STEPS['mol2mol'],
    WORKFLOW_STEPS['candidate_analysis'],
    WORKFLOW_STEPS['mmst'],
    WORKFLOW_STEPS['candidate_analysis'],
    WORKFLOW_STEPS['visual_comparison'],    # old tool or skip if not needed
    WORKFLOW_STEPS['visual_analysis_agent'] # new LLM-based agent
]
The “visual_analysis_agent” will see all the final candidate structures from the prior “candidate_analysis” steps.
It can optionally request more data from “visual_comparison” if that agent is generating images.
7. Putting It All Together
Here’s the concise summary of what you’d do:

Add a new agent file: visual_analysis_agent.py.

Implement in it:

A function to load final candidate data from previous steps (the combined JSON, or in-memory structures).
A function to generate or retrieve images for each candidate (or reference the images from your existing “visual_comparison” step).
A function to build LLM prompts with all relevant data (predicted vs. experimental, any big mismatch flags).
A function to call the LLM and parse the responses.
A function to store or print the final commentary and results in a user-friendly format.
Add an entry for this new agent in your workflow dictionary (WORKFLOW_STEPS['visual_analysis_agent'] = VisualAnalysisAgent()).

Finalize prompt templates, decide whether you want separate steps for each spectrum or one big prompt per candidate.

Test it on a small set of structures. Verify the LLM commentary is helpful and consistent. Possibly refine the prompts if you want more structured or more informal text.

Final Thoughts
This new step (the “visual analysis / LLM-based final agent”) becomes a capstone that merges all your data into an explainable summary. It addresses the final question: “Okay, we have all these candidates with numeric scores—what does a human-like read of their NMR match say?” This approach should give you exactly the deeper interpretative layer you’ve been wanting.